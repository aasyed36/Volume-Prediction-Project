{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5728d366",
   "metadata": {},
   "source": [
    "# Data processing\n",
    "Input: TAQ millisecond data\n",
    "Output: TAQ data grouped by 15 minutes (you can change this easily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f36d3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "TAQ_MS_FILE = \"data/TAQ_Millisecond_AAPL_2023.csv\"\n",
    "chunksize = 2**16\n",
    "converters = {\n",
    "    'SIZE':np.int64,\n",
    "    'PRICE':np.float64,\n",
    "    'TR_CORR':np.int64,\n",
    "    'TR_SEQNUM':np.int64,\n",
    "    'TR_ID':np.int64,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479e3b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is because the TAQ data is too big to load in memory.\n",
    "chunks = []\n",
    "for chunk in pd.read_csv(TAQ_MS_FILE, chunksize=chunksize, converters=converters):\n",
    "    # process each chunk here\n",
    "    chunk['datetime'] = pd.to_datetime(chunk['DATE'] + ' ' + chunk['TIME_M'])\n",
    "    chunk.index = chunk['datetime']\n",
    "    grouped = chunk.groupby(pd.Grouper(freq='15min')).agg(\n",
    "        OPEN      = pd.NamedAgg(column=\"PRICE\", aggfunc=\"first\"),\n",
    "        HIGH      = pd.NamedAgg(column=\"PRICE\", aggfunc=\"max\"),\n",
    "        LOW       = pd.NamedAgg(column=\"PRICE\", aggfunc=\"min\"),\n",
    "        CLOSE     = pd.NamedAgg(column=\"PRICE\", aggfunc=\"last\"),\n",
    "        AVG_PRICE = pd.NamedAgg(column=\"PRICE\", aggfunc=\"mean\"),\n",
    "        VOLUME    = pd.NamedAgg(column=\"SIZE\" , aggfunc=\"sum\"),\n",
    "    )\n",
    "\n",
    "    chunks.append(grouped)\n",
    "    print(chunk['datetime'].iloc[0].strftime('%Y-%m-%d %X'))\n",
    "print(\"Loaded {} data chunks\".format(len(chunks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc107ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat(chunks)\n",
    "print(result.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8310a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, since the chunks we read in do not align with the desired time intervals,\n",
    "# we will have repeated intervals to merge.\n",
    "result = result.groupby(pd.Grouper(freq='15min')).agg({\n",
    "    \"OPEN\"      : \"first\",\n",
    "    \"HIGH\"      : \"max\"  ,\n",
    "    \"LOW\"       : \"min\"  ,\n",
    "    \"CLOSE\"     : \"last\" ,\n",
    "    \"AVG_PRICE\" : \"mean\" ,\n",
    "    \"VOLUME\"    : \"sum\"  ,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5f8f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result[(result.index.dayofweek <= 4) & (result['VOLUME'] > 0)]\n",
    "print(result.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd47a592",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"data/TAQ_15Min_AAPL_2023.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120807c4",
   "metadata": {},
   "source": [
    "## Validation\n",
    "We validate our work against the TAQ Daily set over the same time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d630c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for inspecting the data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68618d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = pd.read_csv(\"data/TAQ_Daily_AAPL_2023.csv\")\n",
    "year.index = pd.to_datetime(year['DATE'])\n",
    "print(year.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f257ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK: These should be the same.\n",
    "plt.plot(year.index, year['avg_buy_price_LR'], label=\"daily_buyprice\")\n",
    "plt.plot(year.index, year['avg_sell_price_LR'], label=\"daily_sellprice\")\n",
    "plt.plot(result.index, result['AVG_PRICE'], label=\"computed_price\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e303863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK: These should be the same.\n",
    "daily_computed_vol = result.groupby(pd.Grouper(freq='1d')).agg({'VOLUME':'sum'})\n",
    "daily_computed_vol = daily_computed_vol[daily_computed_vol['VOLUME'] > 0]\n",
    "\n",
    "plt.semilogy(year.index, year['total_vol'], label=\"daily_vol\")\n",
    "plt.semilogy(daily_computed_vol.index, daily_computed_vol['VOLUME'], label=\"computed_vol\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
