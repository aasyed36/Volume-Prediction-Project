{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499dc212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Change path to\n",
    "TAQ_MS_FILE = \"data/TAQ_Millisecond_AAPL_2023.csv\"\n",
    "chunksize = 2**16  # Size of chunks to read at a time\n",
    "\n",
    "#Converters to ensure data types are correctly handled\n",
    "converters = {\n",
    "    'SIZE': np.int64,\n",
    "    'PRICE': np.float64,\n",
    "    'TR_CORR': np.int64,\n",
    "    'TR_SEQNUM': np.int64,\n",
    "    'TR_ID': np.int64,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da269c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []  #List to hold processed data from each chunk\n",
    "\n",
    "#Read and process the full-sized data file\n",
    "for chunk in pd.read_csv(TAQ_MS_FILE, chunksize=chunksize, converters=converters):\n",
    "    #Create a 'datetime' column by combining 'DATE' and 'TIME_M'\n",
    "    chunk['datetime'] = pd.to_datetime(chunk['DATE'] + ' ' + chunk['TIME_M'])\n",
    "    chunk.set_index('datetime', inplace=True)  #'datetime' is now our index\n",
    "    chunk['TURNOVER'] = chunk['SIZE'] * chunk['PRICE'] #Turnover for each transaction\n",
    "    #Aggregate data by hourly intervals\n",
    "    grouped = chunk.groupby(pd.Grouper(freq='30min')).agg(\n",
    "        TURNOVER=pd.NamedAgg(column=\"TURNOVER\", aggfunc=\"sum\")  #Aggregate turnover\n",
    "    )\n",
    "    chunks.append(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dc5377",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_data = pd.concat(chunks)\n",
    "#Remove duplicates by summing turnovers for each hour again \n",
    "hourly_data = hourly_data.groupby(pd.Grouper(freq='30min')).agg(\n",
    "        TURNOVER=pd.NamedAgg(column=\"TURNOVER\", aggfunc=\"sum\")  #Aggregate turnover\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c11af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets rid of entries outside [9:30 AM, 4:00 PM)\n",
    "hourly_data = hourly_data[~((hourly_data.index.hour < 9) |\n",
    "                            ((hourly_data.index.hour == 9) & (hourly_data.index.minute < 30)) |\n",
    "                            (hourly_data.index.hour >= 16))]\n",
    "# Gets rid of entries outside weekdays\n",
    "hourly_data = hourly_data[hourly_data.TURNOVER > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54490bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rolling median of the trailing 30 days of turnovers\n",
    "hourly_data['date'] = hourly_data.index.date  # Extract date from datetime index\n",
    "daily_turnover = hourly_data.groupby('date')['TURNOVER'].sum()  # Sum turnovers by day\n",
    "rolling_median_30d = daily_turnover.rolling(window=30).median()  # Rolling median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7a3912",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This fn normalizes hourly turnovers by the rolling median of the preceding 30 trading days\n",
    "def normalize_turnovers(df, rolling_median):\n",
    "    normalized_turnovers = []\n",
    "    for date, group in df.groupby(df.index.date):\n",
    "        if date in rolling_median.index:\n",
    "            median_value = rolling_median.loc[date]\n",
    "            normalized_turnovers.extend(group['TURNOVER'] / median_value)\n",
    "        else:\n",
    "            normalized_turnovers.extend([np.nan] * len(group))\n",
    "    return pd.Series(normalized_turnovers, index=df.index)\n",
    "\n",
    "normalized_turnovers = normalize_turnovers(hourly_data, rolling_median_30d)\n",
    "hourly_data['Normalized_TURNOVER'] = normalized_turnovers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3957a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_data = hourly_data.dropna(subset=['Normalized_TURNOVER'])\n",
    "hourly_data.drop(columns=['date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563669bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just a sanity check. Let's find the first row where TURNOVER isn't zero\n",
    "first_non_zero_turnover = hourly_data[hourly_data['TURNOVER'] != 0].iloc[0]\n",
    "first_non_zero_date = first_non_zero_turnover.name.date()\n",
    "\n",
    "#Find and print all turnovers for the first non-zero turnover date (so all the trading hours for that day)\n",
    "non_zero_turnovers_on_date = hourly_data[(hourly_data.index.date == first_non_zero_date) & (hourly_data['TURNOVER'] != 0)]\n",
    "\n",
    "print(\"First non-zero turnover entry:\")\n",
    "print(first_non_zero_turnover)\n",
    "\n",
    "print(f\"All non-zero turnovers for {first_non_zero_date}:\")\n",
    "print(non_zero_turnovers_on_date)\n",
    "\n",
    "#Helps to get the resulting data to as a new CSV file\n",
    "hourly_data.to_csv(\"data/TAQ_30Min_AAPL_2023_normalized.csv\")\n",
    "\n",
    "print(f\"Normalized turnover data saved.\")"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
